#!/usr/bin/env python3
"""
TensorRT vs ONNX å¿«é€Ÿè¨ºæ–·è…³æœ¬
ç”¨æ–¼å°æ¯” ONNX å’Œ TensorRT æ¨¡å‹çš„è¼¸å‡ºå·®ç•°
"""

import numpy as np
import torch
import onnxruntime as ort
import tensorrt as trt
import pycuda.driver as cuda
import pycuda.autoinit
import argparse
from PIL import Image
import torchvision.transforms as T

class TensorRTInfer:
    def __init__(self, engine_path, input_shape, output_shape):
        self.logger = trt.Logger(trt.Logger.WARNING)
        with open(engine_path, "rb") as f, trt.Runtime(self.logger) as runtime:
            self.engine = runtime.deserialize_cuda_engine(f.read())
        
        self.context = self.engine.create_execution_context()
        self.stream = cuda.Stream()
        self.input_shape = input_shape
        self.output_shape = output_shape
        
        # ğŸ”¥ é—œéµä¿®å¾©ï¼šæ˜ç¢ºè¨­å®šè¼¸å…¥å½¢ç‹€
        input_name = self.engine.get_binding_name(0)
        self.context.set_binding_shape(0, input_shape)
        
        # é©—è­‰å½¢ç‹€è¨­å®šæ˜¯å¦æˆåŠŸ
        if not self.context.all_binding_shapes_specified:
            raise RuntimeError("Not all input dimensions specified")
        
        # é‡æ–°è¨ˆç®—è¼¸å‡ºå½¢ç‹€ï¼ˆå› ç‚ºå¯èƒ½æ˜¯å‹•æ…‹çš„ï¼‰
        output_shape_actual = self.context.get_binding_shape(1)
        print(f"ğŸ”§ TensorRT å¼•æ“ä¿¡æ¯:")
        print(f"   è¼¸å…¥å½¢ç‹€: {input_shape}")
        print(f"   æœŸæœ›è¼¸å‡ºå½¢ç‹€: {output_shape}")
        print(f"   å¯¦éš›è¼¸å‡ºå½¢ç‹€: {output_shape_actual}")
        
        # åˆ†é…è¨˜æ†¶é«”
        input_volume = trt.volume(input_shape)
        output_volume = trt.volume(output_shape_actual)
        
        self.h_input = cuda.pagelocked_empty(input_volume, dtype=np.float32)
        self.h_output = cuda.pagelocked_empty(output_volume, dtype=np.float32)
        self.d_input = cuda.mem_alloc(self.h_input.nbytes)
        self.d_output = cuda.mem_alloc(self.h_output.nbytes)
        self.bindings = [int(self.d_input), int(self.d_output)]
        
        # æ›´æ–°å¯¦éš›è¼¸å‡ºå½¢ç‹€
        self.output_shape = output_shape_actual

    def infer(self, image_tensor):
        """
        åŸ·è¡Œ TensorRT æ¨è«–
        
        Args:
            image_tensor: PyTorch tensor, shape=(1, 3, H, W)
        
        Returns:
            numpy array: æ¨è«–çµæœ
        """
        input_np = image_tensor.cpu().numpy().astype(np.float32)
        
        # é©—è­‰è¼¸å…¥å½¢ç‹€
        if input_np.shape != self.input_shape:
            raise ValueError(f"è¼¸å…¥å½¢ç‹€ä¸åŒ¹é…: æœŸæœ› {self.input_shape}, å¯¦éš› {input_np.shape}")
        
        print(f"ğŸ” [TRT] è¼¸å…¥çµ±è¨ˆ: min={input_np.min():.6f}, max={input_np.max():.6f}, mean={input_np.mean():.6f}")
        
        # è¤‡è£½è¼¸å…¥æ•¸æ“š
        np.copyto(self.h_input, input_np.ravel())
        
        # åŸ·è¡Œæ¨è«–
        cuda.memcpy_htod_async(self.d_input, self.h_input, self.stream)
        success = self.context.execute_async_v2(bindings=self.bindings, stream_handle=self.stream.handle)
        
        if not success:
            raise RuntimeError("TensorRT æ¨è«–åŸ·è¡Œå¤±æ•—")
            
        cuda.memcpy_dtoh_async(self.h_output, self.d_output, self.stream)
        self.stream.synchronize()
        
        result = self.h_output.reshape(self.output_shape)
        print(f"ğŸ” [TRT] è¼¸å‡ºçµ±è¨ˆ: min={result.min():.6f}, max={result.max():.6f}, mean={result.mean():.6f}")
        
        return result
    
def get_transform():
    return T.Compose([
        T.ToTensor(),
        T.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),
    ])

def compare_models(onnx_path, engine_path, test_input_type="zeros", test_image_path=None):
    """
    å°æ¯” ONNX å’Œ TensorRT æ¨¡å‹çš„è¼¸å‡º
    
    Args:
        onnx_path: ONNX æ¨¡å‹è·¯å¾‘
        engine_path: TensorRT å¼•æ“è·¯å¾‘
        test_input_type: æ¸¬è©¦è¼¸å…¥é¡å‹ ("zeros", "ones", "random", "image")
        test_image_path: å¦‚æœ test_input_type="image"ï¼ŒæŒ‡å®šåœ–ç‰‡è·¯å¾‘
    """
    
    print("="*60)
    print("ğŸ” TensorRT vs ONNX è¨ºæ–·é–‹å§‹")
    print("="*60)
    
    # 1. æº–å‚™æ¸¬è©¦è¼¸å…¥
    if test_input_type == "zeros":
        test_input = torch.zeros(1, 3, 200, 200)
        print("ğŸ“ ä½¿ç”¨å…¨é›¶å¼µé‡ä½œç‚ºæ¸¬è©¦è¼¸å…¥")
    elif test_input_type == "ones":
        test_input = torch.ones(1, 3, 200, 200)
        print("ğŸ“ ä½¿ç”¨å…¨ä¸€å¼µé‡ä½œç‚ºæ¸¬è©¦è¼¸å…¥")
    elif test_input_type == "random":
        test_input = torch.randn(1, 3, 200, 200)
        print("ğŸ“ ä½¿ç”¨éš¨æ©Ÿå¼µé‡ä½œç‚ºæ¸¬è©¦è¼¸å…¥")
    elif test_input_type == "image" and test_image_path:
        transform = get_transform()
        image = Image.open(test_image_path).convert('RGB').resize((200, 200))
        test_input = transform(image).unsqueeze(0)
        print(f"ğŸ“ ä½¿ç”¨çœŸå¯¦åœ–ç‰‡ä½œç‚ºæ¸¬è©¦è¼¸å…¥: {test_image_path}")
    else:
        raise ValueError("Invalid test_input_type or missing test_image_path")
    
    print(f"ğŸ“Š è¼¸å…¥å¼µé‡çµ±è¨ˆ: shape={test_input.shape}, min={test_input.min():.6f}, max={test_input.max():.6f}")
    
    # 2. ONNX æ¨è«–
    print("\nğŸ”„ åŸ·è¡Œ ONNX æ¨è«–...")
    try:
        ort_session = ort.InferenceSession(onnx_path, providers=['CUDAExecutionProvider', 'CPUExecutionProvider'])
        input_name = ort_session.get_inputs()[0].name
        onnx_output = ort_session.run(None, {input_name: test_input.numpy()})
        onnx_logits = onnx_output[0]
        print(f"âœ… ONNX æ¨è«–æˆåŠŸ")
        print(f"ğŸ“Š ONNX åŸå§‹è¼¸å‡º (logits): shape={onnx_logits.shape}")
        print(f"   Channel 0 stats: min={onnx_logits[0,0].min():.6f}, max={onnx_logits[0,0].max():.6f}, mean={onnx_logits[0,0].mean():.6f}")
        print(f"   Channel 1 stats: min={onnx_logits[0,1].min():.6f}, max={onnx_logits[0,1].max():.6f}, mean={onnx_logits[0,1].mean():.6f}")
        
        # è¨ˆç®— softmax
        exp_logits = np.exp(onnx_logits - np.max(onnx_logits, axis=1, keepdims=True))
        onnx_probs = exp_logits / np.sum(exp_logits, axis=1, keepdims=True)
        onnx_fg_prob = onnx_probs[0, 1]
        print(f"ğŸ“Š ONNX å‰æ™¯æ©Ÿç‡: min={onnx_fg_prob.min():.6f}, max={onnx_fg_prob.max():.6f}, mean={onnx_fg_prob.mean():.6f}")
        
    except Exception as e:
        print(f"âŒ ONNX æ¨è«–å¤±æ•—: {e}")
        return
    
    # 3. TensorRT æ¨è«–
    print("\nğŸ”„ åŸ·è¡Œ TensorRT æ¨è«–...")
    try:
        trt_model = TensorRTInfer(engine_path, (1, 3, 200, 200), (1, 2, 200, 200))
        trt_output = trt_model.infer(test_input)
        print(f"âœ… TensorRT æ¨è«–æˆåŠŸ")
        print(f"ğŸ“Š TensorRT åŸå§‹è¼¸å‡º (logits): shape={trt_output.shape}")
        print(f"   Channel 0 stats: min={trt_output[0,0].min():.6f}, max={trt_output[0,0].max():.6f}, mean={trt_output[0,0].mean():.6f}")
        print(f"   Channel 1 stats: min={trt_output[0,1].min():.6f}, max={trt_output[0,1].max():.6f}, mean={trt_output[0,1].mean():.6f}")
        
        # è¨ˆç®— softmax
        exp_logits_trt = np.exp(trt_output - np.max(trt_output, axis=1, keepdims=True))
        trt_probs = exp_logits_trt / np.sum(exp_logits_trt, axis=1, keepdims=True)
        trt_fg_prob = trt_probs[0, 1]
        print(f"ğŸ“Š TensorRT å‰æ™¯æ©Ÿç‡: min={trt_fg_prob.min():.6f}, max={trt_fg_prob.max():.6f}, mean={trt_fg_prob.mean():.6f}")
        
    except Exception as e:
        print(f"âŒ TensorRT æ¨è«–å¤±æ•—: {e}")
        return
    
    # 4. å°æ¯”çµæœ
    print("\nğŸ“‹ å°æ¯”çµæœ")
    print("="*60)
    
    # å°æ¯”åŸå§‹ logits
    logits_diff = np.abs(onnx_logits - trt_output)
    print(f"ğŸ” åŸå§‹ logits å·®ç•°:")
    print(f"   æœ€å¤§çµ•å°å·®ç•°: {logits_diff.max():.8f}")
    print(f"   å¹³å‡çµ•å°å·®ç•°: {logits_diff.mean():.8f}")
    print(f"   å·®ç•°æ¨™æº–å·®: {logits_diff.std():.8f}")
    
    # å°æ¯”å‰æ™¯æ©Ÿç‡
    prob_diff = np.abs(onnx_fg_prob - trt_fg_prob)
    print(f"ğŸ” å‰æ™¯æ©Ÿç‡å·®ç•°:")
    print(f"   æœ€å¤§çµ•å°å·®ç•°: {prob_diff.max():.8f}")
    print(f"   å¹³å‡çµ•å°å·®ç•°: {prob_diff.mean():.8f}")
    
    # åˆ¤æ–·æ˜¯å¦æ­£å¸¸
    if logits_diff.max() < 1e-5:
        print("âœ… çµæœä¸€è‡´æ€§: æ¥µå¥½ (å·®ç•° < 1e-5)")
    elif logits_diff.max() < 1e-3:
        print("âš ï¸  çµæœä¸€è‡´æ€§: å¯æ¥å— (å·®ç•° < 1e-3)")
    elif logits_diff.max() < 1e-1:
        print("ğŸš¨ çµæœä¸€è‡´æ€§: æœ‰å•é¡Œ (å·®ç•° < 1e-1)")
    else:
        print("ğŸ’¥ çµæœä¸€è‡´æ€§: åš´é‡éŒ¯èª¤ (å·®ç•° >= 1e-1)")
        
    # 5. è¨ºæ–·å»ºè­°
    print("\nğŸ’¡ è¨ºæ–·å»ºè­°:")
    if logits_diff.max() > 1e-3:
        print("   âš ï¸  æª¢æ¸¬åˆ°é¡¯è‘—å·®ç•°ï¼Œå»ºè­°:")
        print("   1. ç¢ºèª TensorRT å¼•æ“æ˜¯å¦ä½¿ç”¨ FP16 (å¯èƒ½å°è‡´ç²¾åº¦æå¤±)")
        print("   2. æª¢æŸ¥è¼¸å…¥é è™•ç†æ˜¯å¦å®Œå…¨ä¸€è‡´")
        print("   3. é©—è­‰ ONNX å°å‡ºæ™‚çš„ opset_version")
        print("   4. å˜—è©¦é‡æ–°æ§‹å»º TensorRT å¼•æ“ (ä½¿ç”¨ FP32)")
    else:
        print("   âœ… æ¨¡å‹è½‰æ›æ­£ç¢ºï¼Œå·®ç•°åœ¨å¯æ¥å—ç¯„åœå…§")
        print("   å¦‚æœä»æœ‰é æ¸¬å•é¡Œï¼Œè«‹æª¢æŸ¥:")
        print("   1. é–¾å€¼è¨­å®š (threshold)")
        print("   2. å¾Œè™•ç†é‚è¼¯")
        print("   3. è©•ä¼°æŒ‡æ¨™è¨ˆç®—")
    
    print("\n" + "="*60)

def main():
    parser = argparse.ArgumentParser(description="TensorRT vs ONNX å¿«é€Ÿè¨ºæ–·å·¥å…·")
    parser.add_argument("--onnx", required=True, help="ONNX æ¨¡å‹è·¯å¾‘")
    parser.add_argument("--engine", required=True, help="TensorRT å¼•æ“è·¯å¾‘")
    parser.add_argument("--test_type", default="zeros", choices=["zeros", "ones", "random", "image"], 
                       help="æ¸¬è©¦è¼¸å…¥é¡å‹")
    parser.add_argument("--test_image", help="å¦‚æœ test_type=imageï¼ŒæŒ‡å®šæ¸¬è©¦åœ–ç‰‡è·¯å¾‘")
    
    opts = parser.parse_args()
    
    compare_models(opts.onnx, opts.engine, opts.test_type, opts.test_image)

if __name__ == "__main__":
    main()